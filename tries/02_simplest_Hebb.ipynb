{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62b42f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442f1e54",
   "metadata": {},
   "source": [
    "### train on handwritten digits dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01d35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and normalize the digits dataset\n",
    "digits = load_digits()\n",
    "X = digits.data.astype(float)\n",
    "X /= X.max()  # scale pixel values to [0,1]\n",
    "y = digits.target\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "num_classes = 10\n",
    "num_features = X_train.shape[1]\n",
    "weights = np.zeros((num_classes, num_features))\n",
    "learning_rate = 0.01\n",
    "\n",
    "predictions = np.argmax(weights @ X_test.T, axis=0)\n",
    "print(f\"Test initilize:\", accuracy_score(y_test, predictions))\n",
    "# Hebbian/perceptron learning\n",
    "for epoch in range(5):\n",
    "    for x_vec, label in zip(X_train, y_train):\n",
    "        activations = weights @ x_vec\n",
    "        pred_class = np.argmax(activations)\n",
    "        # Reinforce correct class and penalize predicted class\n",
    "        weights[label] += learning_rate * x_vec\n",
    "        weights[pred_class] -= learning_rate * x_vec\n",
    "\n",
    "    # Evaluate\n",
    "    predictions = np.argmax(weights @ X_test.T, axis=0)\n",
    "    print(f\"Test accuracy on epoch {epoch}:\", accuracy_score(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95647a8",
   "metadata": {},
   "source": [
    "### train on CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2071d924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.2674\n"
     ]
    }
   ],
   "source": [
    "# Transformation: convert images to tensors and flatten them\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                  # convert PIL image to tensor in [0,1]\n",
    "    transforms.Lambda(lambda x: x.view(-1)) # flatten 3×32×32 image to 3072‑D vector\n",
    "])\n",
    "\n",
    "# Load CIFAR‑10 training and test data\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True,\n",
    "                                transform=transform)\n",
    "test_dataset  = datasets.CIFAR10(root='./data', train=False, download=True,\n",
    "                                transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True)\n",
    "test_loader  = torch.utils.data.DataLoader(test_dataset,  batch_size=256, shuffle=False)\n",
    "\n",
    "num_classes  = 10\n",
    "num_features = 32 * 32 * 3\n",
    "\n",
    "# Initialize weights as a NumPy array for efficiency\n",
    "weights = np.zeros((num_classes, num_features), dtype=np.float32)\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# Training loop (one epoch for simplicity)\n",
    "for epoch in range(1):\n",
    "    for data, target in train_loader:\n",
    "        # data is a tensor of shape [1,3072]; convert to NumPy\n",
    "        x = data.numpy().reshape(-1)\n",
    "        label = target.item()\n",
    "        # Compute activations\n",
    "        activations = weights @ x\n",
    "        pred_class  = activations.argmax()\n",
    "        # Hebbian/perceptron update: reinforce correct class and penalize predicted class\n",
    "        weights[label] += learning_rate * x\n",
    "        weights[pred_class] -= learning_rate * x\n",
    "\n",
    "# Evaluate on the test set\n",
    "correct = 0\n",
    "count   = 0\n",
    "for data, target in test_loader:\n",
    "    x_batch = data.numpy().reshape(data.size(0), -1)  # batch_size × 3072\n",
    "    outputs = weights @ x_batch.T\n",
    "    preds   = outputs.argmax(axis=0)\n",
    "    labels  = target.numpy()\n",
    "    correct += (preds == labels).sum()\n",
    "    count   += labels.shape[0]\n",
    "print(f\"Test accuracy: {correct / count:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4221c1e4",
   "metadata": {},
   "source": [
    "### improve model by Using Oja’s/GHA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b089872",
   "metadata": {},
   "source": [
    "$\\mathbf{w}_j \\leftarrow \\mathbf{w}_j + \\eta\\,y_j \\left(\\mathbf{x} - \\sum_{k\\leq j} y_k \\mathbf{w}_k\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d3944",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "851d1c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9527777777777777\n"
     ]
    }
   ],
   "source": [
    "# Load and normalize data\n",
    "X, y = load_digits(return_X_y=True)\n",
    "X = X.astype(float)\n",
    "X /= X.max()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Generalized Hebbian Algorithm (GHA)\n",
    "num_components = 32\n",
    "lr = 0.005\n",
    "W = np.random.randn(num_components, X_train.shape[1]) * 0.01\n",
    "\n",
    "for epoch in range(15):\n",
    "    for x in X_train:\n",
    "        y_out = W @ x\n",
    "        for j in range(num_components):\n",
    "            # reconstruction using already learned components\n",
    "            recon = np.sum([y_out[k] * W[k] for k in range(j+1)], axis=0)\n",
    "            W[j] += lr * y_out[j] * (x - recon)\n",
    "# normalize\n",
    "W /= np.linalg.norm(W, axis=1, keepdims=True)\n",
    "\n",
    "# Project data and train classifier\n",
    "X_train_proj = X_train @ W.T\n",
    "X_test_proj  = X_test  @ W.T\n",
    "clf = LogisticRegression(max_iter=1000)\n",
    "clf.fit(X_train_proj, y_train)\n",
    "print('Test accuracy:', accuracy_score(y_test, clf.predict(X_test_proj)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2cf752",
   "metadata": {},
   "source": [
    "### on cifar-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06980a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import math, random, time\n",
    "from dataclasses import dataclass\n",
    "# -------------------------------\n",
    "# Utilities\n",
    "# -------------------------------\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed); torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "@dataclass\n",
    "class HebbCfg:\n",
    "    eta: float = 0.05          # Hebbian learning rate for the trace update\n",
    "    trace_decay: float = 0.9   # decay for running Hebbian trace (0=no decay, 1=keep)\n",
    "    bp_through_plasticity: bool = True  # allow gradients through H updates\n",
    "    clip_h: float | None = 1.0 # clip Hebbian trace for stability (None to disable)\n",
    "    act: str = \"tanh\"          # nonlinearity to bound post-activity for Hebb (\"tanh\" or \"identity\")\n",
    "\n",
    "# -------------------------------\n",
    "# Plastic Linear Layer\n",
    "# W_eff = W + alpha ⊙ H\n",
    "# H_{t+1} = trace_decay * H_t + eta * (y_hat ⊗ x)\n",
    "# where y_hat is bounded post-activation (e.g., tanh(pre-softmax logits) or pre-activation).\n",
    "# alpha is learnable (either scalar or per-weight)\n",
    "# -------------------------------\n",
    "class PlasticLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, cfg: HebbCfg,\n",
    "                 learn_alpha=True, alpha_init=0.0, per_weight_alpha=True, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.cfg = cfg\n",
    "\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        self.bias = nn.Parameter(torch.zeros(out_features)) if bias else None\n",
    "\n",
    "        if per_weight_alpha:\n",
    "            self.alpha = nn.Parameter(torch.full((out_features, in_features), alpha_init)) if learn_alpha \\\n",
    "                         else torch.full((out_features, in_features), alpha_init, requires_grad=False)\n",
    "        else:\n",
    "            self.alpha = nn.Parameter(torch.tensor(alpha_init)) if learn_alpha \\\n",
    "                         else torch.tensor(alpha_init, requires_grad=False)\n",
    "\n",
    "        # Hebbian trace H (fast weights): same shape as weight\n",
    "        self.register_buffer(\"H\", torch.zeros(out_features, in_features))\n",
    "\n",
    "        # Init weights (Kaiming)\n",
    "        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n",
    "\n",
    "        if bias:\n",
    "            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_in)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def reset_hebb(self):\n",
    "        self.H.zero_()\n",
    "\n",
    "    def _bound_post(self, y_pre):\n",
    "        if self.cfg.act == \"tanh\":\n",
    "            return torch.tanh(y_pre)\n",
    "        elif self.cfg.act == \"identity\":\n",
    "            return y_pre\n",
    "        else:\n",
    "            raise ValueError(\"Unknown act\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Effective weights = baseline + plastic component\n",
    "        H_used = self.H.detach()\n",
    "        alpha = self.alpha\n",
    "        if alpha.dim() == 0:\n",
    "            eff_w = self.weight + alpha * H_used\n",
    "        else:\n",
    "            eff_w = self.weight + alpha * H_used  # broadcast matches (out, in)\n",
    "\n",
    "        y = F.linear(x, eff_w, self.bias)\n",
    "\n",
    "        # Update Hebbian trace (online) **during forward** for next step\n",
    "        # Use bounded post-activation y_hat to avoid runaway values.\n",
    "        with torch.enable_grad():\n",
    "            # If we don't want to backprop through plasticity, detach x or y_hat (or both)\n",
    "            x_for_hebb = x if self.cfg.bp_through_plasticity else x.detach()\n",
    "            y_hat = self._bound_post(y) if self.cfg.bp_through_plasticity else self._bound_post(y.detach())\n",
    "\n",
    "            # Batch outer product: average over batch for stability\n",
    "            # y_hat: [B, out], x: [B, in] => H_delta: [out, in]\n",
    "            H_delta = (y_hat.unsqueeze(2) * x_for_hebb.unsqueeze(1)).mean(dim=0)\n",
    "\n",
    "        # Hebbian decay + add new outer product\n",
    "        # IMPORTANT: do NOT wrap in no_grad; we want the option to backprop through H if enabled.\n",
    "        # But updating a buffer in graph requires a trick; we keep H as buffer and update out-of-graph.\n",
    "        # So when bp_through_plasticity=True, gradients flow into eff_w via current H usage, not into H itself.\n",
    "        with torch.no_grad():\n",
    "            self.H.mul_(self.cfg.trace_decay).add_(self.cfg.eta * H_delta)\n",
    "            if self.cfg.clip_h is not None:\n",
    "                self.H.clamp_(-self.cfg.clip_h, self.cfg.clip_h)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "713c95d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------\n",
    "# Simple ConvNet feature extractor + plastic classifier head\n",
    "# -------------------------------\n",
    "class HebbNet(nn.Module):\n",
    "    def __init__(self, hebb_cfg: HebbCfg, hidden=256, plastic_layers=1):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 16x16\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 8x8\n",
    "        )\n",
    "        self.flatten_dim = 128 * 8 * 8\n",
    "\n",
    "        self.hebb_cfg = hebb_cfg\n",
    "        self.plastic_layers = plastic_layers\n",
    "\n",
    "        if plastic_layers == 2:\n",
    "            self.fc1 = PlasticLinear(self.flatten_dim, hidden, hebb_cfg, learn_alpha=True, alpha_init=0.0, per_weight_alpha=False)\n",
    "            self.fc2 = PlasticLinear(hidden, 10, hebb_cfg, learn_alpha=True, alpha_init=0.0, per_weight_alpha=False)\n",
    "        elif plastic_layers == 1:\n",
    "            self.fc1 = nn.Linear(self.flatten_dim, hidden)\n",
    "            self.fc2 = PlasticLinear(hidden, 10, hebb_cfg, learn_alpha=True, alpha_init=0.0, per_weight_alpha=False)\n",
    "        else:\n",
    "            raise ValueError(\"plastic_layers must be 1 or 2\")\n",
    "\n",
    "    def reset_hebb(self):\n",
    "        # Reset all plastic layers\n",
    "        if isinstance(self.fc1, PlasticLinear): self.fc1.reset_hebb()\n",
    "        if isinstance(self.fc2, PlasticLinear): self.fc2.reset_hebb()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        if isinstance(self.fc1, PlasticLinear):\n",
    "            x = self.fc1(x)\n",
    "        else:\n",
    "            x = self.fc1(x)\n",
    "        x = F.relu(x, inplace=True)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# -------------------------------\n",
    "# Training\n",
    "# -------------------------------\n",
    "def get_loaders(data_root=\"./data\", batch_size=128, num_workers=4):\n",
    "    mean = (0.4914, 0.4822, 0.4465)\n",
    "    std = (0.2470, 0.2435, 0.2616)\n",
    "    train_tf = T.Compose([\n",
    "        T.RandomHorizontalFlip(),\n",
    "        T.RandomCrop(32, padding=4),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "    test_tf = T.Compose([\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean, std),\n",
    "    ])\n",
    "\n",
    "    train_ds = torchvision.datasets.CIFAR10(root=data_root, train=True, download=True, transform=train_tf)\n",
    "    test_ds = torchvision.datasets.CIFAR10(root=data_root, train=False, download=True, transform=test_tf)\n",
    "\n",
    "    train_ld = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "    test_ld = DataLoader(test_ds, batch_size=256, shuffle=False, num_workers=num_workers, pin_memory=True)\n",
    "    return train_ld, test_ld\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        correct += (pred == y).sum().item()\n",
    "        total += y.numel()\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55c0bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(123)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Hebbian/backprop config ---\n",
    "hebb_cfg = HebbCfg(\n",
    "    eta=0.05,                 # Hebbian update step\n",
    "    trace_decay=0.95,         # decay of Hebbian trace\n",
    "    bp_through_plasticity=True,  # let grads flow through plastic path\n",
    "    clip_h=1.0,\n",
    "    act=\"tanh\",               # bound post-activity for stability\n",
    ")\n",
    "\n",
    "# --- Data ---\n",
    "train_ld, test_ld = get_loaders(batch_size=128)\n",
    "\n",
    "# --- Model ---r\n",
    "model = HebbNet(hebb_cfg, hidden=256, plastic_layers=1).to(device)\n",
    "\n",
    "# --- Optimizer (learn baseline weights + alpha) ---\n",
    "# Weight decay only on baseline weights and biases; exclude alpha if you want\n",
    "decay, no_decay = [], []\n",
    "for n, p in model.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        if n.endswith(\"weight\") or n.endswith(\"bias\"):\n",
    "            decay.append(p)\n",
    "        else:\n",
    "            # alpha etc.\n",
    "            no_decay.append(p)\n",
    "optim = torch.optim.AdamW([\n",
    "    {\"params\": decay, \"weight_decay\": 5e-4},\n",
    "    {\"params\": no_decay, \"weight_decay\": 0.0},\n",
    "], lr=3e-4, betas=(0.9, 0.999))\n",
    "\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingLR(optim, T_max=50)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da3e255f",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [10, 256]] is at version 4; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m loss = criterion(logits, y)\n\u001b[32m     16\u001b[39m optim.zero_grad(set_to_none=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), \u001b[32m1.0\u001b[39m)\n\u001b[32m     19\u001b[39m optim.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/yukihara/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/yukihara/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/envs/yukihara/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mRuntimeError\u001b[39m: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [10, 256]] is at version 4; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)."
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Train ---\n",
    "epochs = 30\n",
    "best_acc = 0.0\n",
    "for ep in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    # Episodic behavior: reset Hebbian trace each epoch (you can reset per batch instead)\n",
    "    model.reset_hebb()\n",
    "\n",
    "    t0 = time.time()\n",
    "    for x, y in train_ld:\n",
    "        x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "\n",
    "        optim.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optim.step()\n",
    "\n",
    "        acc = evaluate(model, test_ld, device)\n",
    "        print(f\"Epoch {ep:02d} | train loss: {loss.item():.4f} | test acc: {acc*100:5.2f}%\")\n",
    "\n",
    "    sched.step()\n",
    "    train_time = time.time() - t0\n",
    "\n",
    "    # Eval\n",
    "    acc = evaluate(model, test_ld, device)\n",
    "    best_acc = max(best_acc, acc)\n",
    "    print(f\"Epoch {ep:02d} | test acc: {acc*100:5.2f}% | best: {best_acc*100:5.2f}% | time {train_time:5.1f}s\")\n",
    "\n",
    "print(\"Done. Best accuracy:\", best_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ad8af2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yukihara",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
